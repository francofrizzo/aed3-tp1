\section{Ejercicio 3}
    % 1. Describir detalladamente el problema a resolver dando ejemplos del mismo y sus soluciones.
    \subsection{Descripción del problema}

        Gokú se está enfrentando a N androides y necesita destruirlos con la menor cantidad de Kamehamehas posibles. Los enemigos ode Gokú se encuentran en posiciones $(X_i , Y_i)$ y los Kamehameha recorren una semirrecta desde donde Gokú lo lance, en cualquier dirección que Gokú lo decida. ¿Cuántos Kamehamehas necesita Gokú para desturir a todos los androides del doctor Maki Gero?

        Se pide escribir un algoritmo que tome la cantidad de androides N y las posiciones $(X_i , Y_i)$ de los mismos y decida cuántos Kamehameha debe lanzar Gokú y a qué enemigos destruye con cada Kamehameha. Si hay más de una solución óptima, el algoritmo puede devolver cualquiera de ellas. Se pide utilizar la técnica de Backtracking y elaborar podas y estrategias para mejorar los tiempos de ejecución; éstas deberán estar apropiadamente documentadas en el informe. El algoritmo debe tener una complejidad temporal $O(N^{N+2})$ o mejor.

        La salida que el algoritmo debe retornar consiste en la cantidad de kamehamehas seguido de esa cantidad de lineas, donde cada una comienza con lacantidad de androides destruidos, seguido por los indices de dichos androides.

        Por ejemplo, para la entrada:
        
        \begin{verbatim}
        5
        0 0
        0 1
        0 2
        1 2
        2 2
        \end{verbatim}
        Una posible salida sería:

        \begin{verbatim}
        2
        3 3 4 5
        2 1 2
        \end{verbatim}

        Observación: Los kamehamehas matan a los androides. Esto significa que si un androide esta muerto, por mas que un kamehameha vuelva a pasar por encima de él, no vuelve a morir. Es por eso que:

        \begin{verbatim}
        2
        3 3 4 5
        3 1 2 3
        \end{verbatim}

        no es una salida válida, ya que estaría matando por segunda vez al androide ubicado en la posicion (0,2).
        

    % 2. Explicar de forma clara, sencilla, estructurada y concisa, las ideas desarrolladas para la resolución del problema. Utilizar pseudocódigo y lenguaje coloquial (no código fuente). Justificar por qué el procedimiento resuelve efectivamente el problema.
    \subsection{Solución propuesta}

    La solución consiste en probar todas las combinaciones posibles de kamehamehas, guardando en cada uno de ellos los grupos de androides a quienes destruye, y retornar la lista de grupos de androides de menor tamaño. Para hacer esto utilizamos la técnica de Backtracking; se llama recursivamente a una función que se encarga de probar cada kamehameha posible y para cada uno ramificarse.

    Las combianaciones de kamehamehas estan dadas por los pares de androides posibles. Lanzar un kamehameha consiste en tomar la recta que pasa por dos androides elegidos. Luego se almacenan los androides que son destruidos por ese disparo y se continua con la ramificación de kamehamehas.

    Una vez finalizada la ejecución del algoritmo, se habrán recorrido todas las posibles combinaciones de disparos. La solucion a este problema es la cual tenga menor cantidad de llamadas recursivas, ya que eso implica una menor cantidad de kamehamehas.

    \subsubsection{Detalles implementativos}
    
    El algoritmo fue implementado en lenguaje C++. Para almacenar la solución, se recurre a la clase \texttt{vector}, proporcionada por la librería estándar del lenguaje.

    Durante la ejecución del algoritmo se utilizan dos variables globales; un entero para almacenar la cantidad de enemigos y un vector que contiene la ubicación de cada uno de ellos. La ejecución tiene como función principal un método recursivo. Analizemos este metodo:

    \begin{itemize}
        \item Se prueban todos los pares de enemigos posibles mediante dos ciclos anidados, de forma que si un kamehameha equivale a un par de enemigos se prueben todos los kamehameha posibles
        \item Dentro del ciclo interior se revisa a qué enemigos destruye el kamehameha descripto por el par de enemigos elegido y se destruyen esos enemigos para luego agregarlos a la solucion
        \item Destruir los enemigos consiste en eliminarlos de un vector que contiene a los enemigos restantes. Este vector es una copia del vector de enemigos restantes, ya que para cada kamehameha distinto, el algoritmo se bifurca con enemigos restantes distintos
        \item Agregarlos a la solución consiste en añadir un vector que contiene a dichos enemigos a un vector de vectores, que representa la solucion: cada posición del vector es un kamehameha distinto conteniendo a los enemigos que destruye
        \item Tiene como casos base la llamada en donde la cantidad de enemigos restantes es igual o menor a 2, en estos casos lo que se hace es agregar estos ultimos enemigos que quedaron a la solución, como parte del último kamehameha lanzado, y terminar la ejecución del método
    \end{itemize}

    La primer poda que hicimos fue la de no probar todos los kamehamehas posibles. El kamehameha que va desde el andoride $a_1$ al androide $a_2$ es el mismo que va del $a_2$ al $a_1$. Esto lo hicimos usando dos ciclos en donde el exterior (correspondiente a $a_1$) recorría todos los androides, y el interior (correspondiente a $a_2$) recorría los androides de $a_1$ en adelante. De esta forma se logra evitar la repetición innecesaria de pares de androides.

    \begin{codesnippet}
    \begin{verbatim}
        for (int i = 0; i < cantidadDeAndroides - 1; i++){
            for (int j = i; j < cantidadDeAndroides; j++){
                lanzarKamehameha(i, j);
            }
        }
    \end{verbatim}
    \end{codesnippet}

    Pero luego llegamos a una mejor poda. Si bien ya no repetimos pares de androides, todavía existen algunos kamehamehas superpuestos; este es el caso en que elijo un par de andorides y hay un tercero en la misma recta. Si yo tomo cualquier par de androides entre un conjunto en el que todos pasan por la misma recta, estaría tomando el mismo kamehameha.

    Ejemplo: El kamehameha que pasa por (0,0) y (1,1) es el mismo que pasa por (1,1) y (2,2).

    Lo que hicimos fue almacenar cada disparo para así verificar (recorriendo los kamehamehas guardados) que el par de androides que describen nuestro nuevo kamehameha no haya sido visitado en esta rama de ejecución (podría usarse ese mismo kamehameha pero en una rama diferente).

    Otra poda hecha es la de almacenar la cantidad minima de kamehamehas con la cual el algoritmo llegó efectivamente a una solución (hasta ese momento la mejor), para que si en alguna rama se supera esa cantidad, no seguir ese camino, ya que no conducirá a una solución mejor que la antes encontrada.
       
    % hecho. Deducir una cota de complejidad temporal del algoritmo propuesto y justificar por qué el algoritmo cumple la cota dada. Utilizar el modelo uniforme.
    \subsection{Complejidad teórica}

    Al comienzo se inicializa un vector de $N$ posiciones, lo cual tiene costo de orden $N$. Principalmente se utiliza una función recursiva, la cual determina la complejidad temporal de la solución.
    
    Dicha función contiene dos ciclos, un dentro del otro, en donde el ciclo exterior recorre indices $i$ desde 0 hasta $N - 1$, y el interior desde $i$ hasta $N$. Esto implica que lo implementado dentro del ciclo interior se ejecute $N \times (N - 1)$ veces.

    Analizemos el ciclo interior:

    \begin{itemize}
        \item Se recorren todos los kamehamehas anteriormente lanzados, que a lo sumo pueden ser $N/2$ ya que puedo eliminar al menos 2 androides con un mismo kamehameha, lo cual tiene costo de orden $N$.
        \item Se destruyen los enemigos alcanzados por el kamehameha, esto consiste en recorrerlos todos y borrar cada uno del vector donde estan almacenados, con costo de orden $N^2$ y se los agrega a un vector con enemigos destruidos con costo de orden $N$. Por lo tanto el costo total es de orden $N^2$.
        \item Por último se llama a la función nuevamente.
    \end{itemize}

    Se debe observar que al hacer el nuevo llamado, la cantidad de enemigos va a haber disminuido al menos en 2. Debido a esto, dentro de la siguiente llamada, el ciclo exterior recorrerá $N - 2$ indices y el interior $N - 3$. Así estas cantidades disminuirán de a 2 hasta terminar, llegando a 1.

    Si pensamos la cantidad de llamadas recursivas como un árbol donde cada node es una llamada, tenemos un árbol de a lo sumo $N/2$ niveles, donde en su primer nivel hay $N \times (N - 1)$ llamados. Cada uno tiene $(N - 2) \times (N - 3)$ hijos, a su vez cada uno de ellos $(N - 4) \times (N - 5)$ y así sucesivamente hasta llegar a 1. Por lo que en el segundo nivel habrán $N \times (N - 1) \times (N - 2) \times (N - 3)$ nodos, y de esta forma en el último nivel se encontraran $N!$ hojas.

    El árbol no puede tener más de $N/2$ niveles ya que cada nivel representa un kamehameha, y no pueden haber más de $N/2$ kamehamehas.

    Ejemplo para $N$ = 4: 

    <ÁRBOL>

    En el nivel $k$ con $0 \leq k \leq N / 2$ se puede ver que cada nodo tiene $(N - 2k) \times (N - 2k - 1)$ hijos,
    debido a los ciclos explicados previamente. Entonces la cantidad de nodos en el nivel $k$ está dada por la cantidad de nodos en $k - 1$ multiplcado por $(N - 2k) \times (N - 2k -1)$. De esta forma la cantidad de nodos en el nivel $k$ es igual al producto de las cantidades de hijos por nodo de cada nivel, hasta $k$. Es decir, 

    \[\text{\#nodos en nivel }k = \prod_{l = 0}^{k}(N - 2l) \times (N - 2l -1)\]

    Y por lo tanto se cumple para el último nivel ($k = N/2$) donde los nodos son hojas

    \[\text{\#hojas }= \prod_{l = 0}^{N / 2}(N - 2l) \times (N - 2l -1) = N!\]

    Sabiendo que la cantidad de hojas es $N!$ y la altura es $N/2$, y que cada llamada tiene un costo de orden $N^2$, se puede pensar que este arbol de ejecución está acotado superiormente por una "matriz" de $N/2$ filas y $N!$ columnas donde cada elemento es una llamada de costo $N^2$.

    Es decir, la complejidad del algoritmo está acotada superiormente por $N/2 \times N! \times N^2$ que es de orden $N^3 \times N!$.

    Ahora veremos que vale $N^k \times N! \leq k! \times N^N$ $\forall k$ constante, particularmente lo vale para $k = 3$, y que por lo tanto $N^3 \times N!$ está acotado superiormente por $N^N$ que a su vez está acotado por $N^{N + 2}$, por lo que el algortimo cumple la complejidad requerida.

    \[ N^N = \prod_{i = 0}^{n - 1}n = N^k \times \prod_{i = 0}^{N - k - 1} N \geq N^k \times \prod_{i = 0}^{N - k - 1} (N - 1) = N^k \times \frac{N!}{k!}\]

    Dado que $N^N \geq N^k \times \frac{N!}{k!} \implies k! \times N^N \geq N^k \times N!$ y por lo tanto $N^k \times N! \in \ord(N^N) \forall k \in \nat$

    % 4. Dar un código fuente claro que implemente la solución propuesta. Se deben incluir las partes relevantes del código como apéndice del informe impreso entregado.

    % 5. Realizar una experimentación computacional para medir la performance del programa implementado. Usar un conjunto de casos de test en función de los parámetros de entrada, con instancias aleatorias e instancias particulares (de peor/mejor caso en tiempo de ejecución, por ejemplo). Presentar en forma gráfica una comparación entre los tiempos medidos y la complejidad teórica calculada y extraer conclusiones.
    \subsection{Experimentación}

        Al igual que con los otros dos ejercicios, se realizaron pruebas experimentales para verificar que el tiempo de ejecución del algoritmo cumpliera con la cota asintótica de $\ord(N^{N+2})$, teóricamente demostrada para el peor caso. Se realizaron dos tipos de pruebas:
        
        \begin{itemize}
            \item Pruebas con instancias con características particulares, más específicamente, para el mejor caso, el peor caso y casos intermedios.
            \item Pruebas con instancias generadas aleatoriamente, para obtener una aproximación al comportamiento del algoritmo en el caso promedio.
        \end{itemize}

        \subsubsection{Instancias particulares}

            Todas las instancias utilizadas para estas pruebas se generaron de manera aleatoria, pero restringiendo los resultados obtenidos para cumplir con determinadas características. A continuación se enumeran los criterios tenidos en cuenta para la generación de los escenarios de prueba.

            \begin{itemize}
                \item \textbf{Mejor caso:} El mejor caso del algoritmo se produce cuando todos los androides a destruir están ubicados sobre una única línea recta. En este caso, es seguro que el primer kamehameha con el que se intente será la solución óptima del problema, por lo que solo se bajará por esa rama de la recursión.

                \item \textbf{Peor caso:} El peor caso del algoritmo se da cuando no existen tres androides alineados a destruir. En consecuencia, cada kamehameha destruirá tan solo dos enemigos, que es el mínimo posible. Esto maximiza la complejidad temporal del algoritmo por dos motivos. Por un lado, en cada nivel de la recursión, se intentarán disparar todos los kamehamehas posibles, con la esperanza de que pueda encontrarse una solución más razonable. Por otra parte, en cada llamada recursiva que se efectúe, la entrada solo se reducirá en dos elementos, haciendo que la profundidad de la recursión alcance siempre la cota máxima de $\frac{N}{2}$.

                \item \textbf{Caso intermedio:} También se agregó un escenario de prueba adicional, con el objetivo principal de poner a prueba la efectividad de las podas implementadas. El mismo consiste en la combinación de dos instancias de mejor caso de tamaño $\frac{N}{2}$; es decir, la mitad de los androides se encuentran sobre una línea recta y la otra mitad, sobre una recta diferente. Las coordenadas de los enemigos son mezcladas de forma aleatoria para evitar que el kamehameha óptimo sea necesariamente el primero que se intente.
                
                Bajo estas condiciones, cabe esperar que las podas entren en acción, reduciendo considerablemente cantidad de llamadas recursivas que se ejecutan completas y consiguiendo un rendimiento apreciablemente superior al observado en el peor caso.

                Cabe destacar que este es un simple caso adicional de prueba, concebido con el objetivo de ilustrar la gran variación en el rendimiento del algoritmo según las características de los datos de entrada, pero que no guarda relación alguna con el comportamiento del algoritmo en el caso promedio.
            \end{itemize}

            Para los tres escenarios previstos, se generaron instancias de prueba para todos los valores de $N$ entre $1$ y $13$. Luego, en cada caso, se ejecutaron $60$ repeticiones del algoritmo, midiendo cada vez el tiempo de ejecución y tomando luego el promedio entre los resultados obtenidos.

            En un principio, se descubrió que se producían picos en el tiempo de ejecución en las primeras corridas del programa, tendiendo los valores a estabilizarse con las sucesivas corridas. Si bien no se pudo determinar con precisión el origen de estas anomalías, se decidió, para minimizar la varianza de los datos obtenidos, tratar a las primeras $20$ repeticiones de cada instancia como \emph{outliers}, y promediar solo los valores de las últimas $40$.

            \renewcommand\constante{0.001}

            Los resultados obtenidos se exponen en el gráfico de la Figura \ref{fig:exp3:part_tiempo_base}, donde se ilustra también la cota teórica de $c \times N^{N + 2}$ (el valor de $c$ utilizado es \constante). Puede observarse claramente que, incluso en el peor escenario, el tiempo de ejecución del algoritmo tiende a aumentar a un ritmo estrictamente más lento que el de la cota prevista.

            \begin{figure}[H]
                \centering
                \caption{}
                \label{fig:exp3:part_tiempo_base}
                \begin{tikzpicture}
                    \begin{axis}[
                            title={},
                            xlabel={Tamaño de entrada ($N$)},
                            ylabel={Tiempo de ejecución (nanosegundos)},
                            ymode = log,
                            scaled x ticks=false,
                            scaled y ticks=false,
                            enlargelimits=0.05,
                            width=0.5\textwidth,
                            height=0.5\textwidth,
                            legend pos=north west,
                            legend cell align=left,
                            xmin=1
                        ]

                        \addplot[color=red] table[x index=0,y index=1]{../exp/kamehamehaPeor};
                        \addplot[color=blue] table[x index=0,y index=1]{../exp/kamehamehaIntermedio};
                        \addplot[color=green] table[x index=0,y index=1]{../exp/kamehamehaMejor};
                        \addplot[color=black] table[x index=0, y expr={\constante * (x^(x+2))}]{../exp/kamehamehaPeor};
                        \legend{$T_P(N)$, $T_M(N)$, $T_I(N)$, $c \times N^{N+2}$}
                    \end{axis}
                \end{tikzpicture}
            \end{figure}

            En la Figura \ref{fig:exp3:part_tiempo_sobre_exp} se muestra el cociente entre los datos obtenidos y la función $\times N^{N + 2}$ (se considera el mismo valor de $c$ que en el gráfico anterior). Puede observarse como, al aumentar el tamaño de $N$, el cociente se aproxima rápidamente a $0$, ilustrando el hecho de que el crecimiento asintótico de la cota es estrictamente mayor que el de los tiempos obtenidos en las mediciones.

            \begin{figure}[H]
                \centering
                \caption{}
                \label{fig:exp3:part_tiempo_sobre_exp}
                \begin{tikzpicture}
                    \begin{axis}[
                            title={},
                            xlabel={Tamaño de entrada ($N$)},
                            ylabel={Tiempo de ejecución (nanosegundos)},
                            ymode = log,
                            scaled x ticks=false,
                            scaled y ticks=false,
                            enlargelimits=0.05,
                            width=0.5\textwidth,
                            height=0.5\textwidth,
                            legend pos=north west,
                            legend cell align=left,
                            xmin=1
                        ]

                        \addplot[color=red] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaPeor};
                        \addplot[color=blue] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaIntermedio};
                        \addplot[color=green] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaMejor};
                        \addplot[color=black] table[x index=0, y expr={\constante}]{../exp/kamehamehaPeor};
                        \legend{$\frac{T_P(N)}{N^{N+2}}$, $\frac{T_M(N)}{N^{N+2}}$, $\frac{T_I(N)}{N^{N+2}}$, $c$}
                    \end{axis}
                \end{tikzpicture}
            \end{figure}

            El análisis expuesto de los datos recopilados presenta evidencia empírica sobre la pertinencia la cota de complejidad demostrada teóricamente. Más aún, permite llegar a la conclusión que, incluso en las instancias de peor caso, esta cota resulta holgada, es decir, que la complejidad del algoritmo presentado es estrictamente $\ord(N^{N+2})$.

        \subsubsection{Instancias aleatorias}
